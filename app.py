# -*- coding: utf-8 -*-
"""app.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DH4j_6DHHqARJXtyiTrxM7PQ-vHngrph
"""

# Step 1: Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Step 2: Import libraries
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import streamlit as st  # Streamlit won't work directly on Colab, so this can be omitted if you're not deploying it locally.

# Step 3: Load CSV files from Google Drive
# Replace 'YourFolder' with the actual folder name where your files are located in Google Drive.
confusion_matrix_data = pd.read_csv('/content/drive/MyDrive/Teachnook major project/confusion_matrix.csv')
feature_importance_data = pd.read_csv('/content/drive/MyDrive/Teachnook major project/feature_importance.csv')
model_performance_data = pd.read_csv('/content/drive/MyDrive/Teachnook major project/model_performance_metrics.csv')

# Step 4: Clean up the confusion matrix data
confusion_matrix_data_cleaned = confusion_matrix_data.set_index('Unnamed: 0')
confusion_matrix_data_cleaned.columns = ['No Failure', 'Failure']

# Step 5: Clean up the model performance data
model_performance_data_cleaned = model_performance_data.rename(columns={'Unnamed: 0': 'Class'}).set_index('Class')

# Step 6: Plot Confusion Matrix
plt.figure(figsize=(6, 4))
sns.heatmap(confusion_matrix_data_cleaned, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.tight_layout()
plt.show()

# Step 7: Plot Feature Importance
plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=feature_importance_data, palette='viridis')
plt.title('Feature Importance - Random Forest')
plt.tight_layout()
plt.show()

# Step 8: Plot Model Performance Metrics (Precision, Recall, F1-Score)
plt.figure(figsize=(8, 5))
model_performance_data_cleaned[['precision', 'recall', 'f1-score']].plot(kind='bar')
plt.title('Precision, Recall, F1-Score by Class')
plt.xticks(ticks=[0, 1], labels=['No Failure (0)', 'Failure (1)'])
plt.ylabel('Score')
plt.tight_layout()
plt.show()

# Step 9: Display Model Accuracy (hardcoded for now, replace with your actual accuracy metric if available)
accuracy = 88  # Example accuracy
print(f"Model Accuracy: {accuracy}%")